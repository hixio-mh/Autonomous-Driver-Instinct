# Autonomous-Driver-Instinct
This is the Project Proposal for the Final Year Project. The project will be based on creating Engaged Live Feed for safe and autonomous vehicle operability. Throughout the project we’ll be working on various models (listed: Face Detection, Driver’s Drowsiness detection via Facial Landmark Detection for blinking and Yawning, Eye on the road detection via Human Pose Estimation, Driver Recognition and Gaze Estimation). The project will operate through a camera fed which will be treated as input for various computation and inference to be performed and process out the final Live Stats Feed. It is core conceptualized for security and safety so the project will perform all the operation only for an authorized driver and deny all the control if an unauthorized driver sits on driver seat. The final project will be capable of operating on an edge device(like a camera connected to Intel Neural Compute Stick) where we’ll be just required to install our code on that device simple but our focus through out the project will be to present on local machine because of high cost of edge device that is needless to invest on for this project. The entire code base will be written on Python.  The project will examine the driver condition and based on it will generate Live Feed of Information inferenced in the RealTime. The feed will be showcasing the blink count, blink time, driver’s Yawn count, Yawn time and will issue the Danger Alarm once the feed data exceeds the set threshold permissible.Further more detecting the angle of sight, it will generate alerts like “Keep your Eyes on the Road” or “STOP looking at your Phone or gazing at anywhere other than steering or road. Once it’ll be done the project will then shift to the Authentication Build Portion where we’ll be implementing Driver Recognition Model; the idea is to assign a directory with the image of permissible drivers for a particular registered vehicle by the owner. Afterwards, whenever a person will sit on driver seat infront of camera, he/she will first be recognized as authorized or unauthorized. Owner can add a new authorized driver by adding a new face to the authorized_face directory and then parsing it to .json via script to be written; the command will register and recognize the authorized drivers from next time and only after successful authorization will grant permission to the other features.  The broader objective is to develop the project on an edge device in the future version so in the later stage of Project Timeline we’ll be integrating our project with Intel IoT DevCloud for benchmarking and Performance Analysis. We’ll run the inference process on different hardware like CPU, IGPU, Xeon Series, FPGA, VPU and Neural Compute Stick to deduce the perfect inference timing, FPS and Model accuracy to be deploy on suitable edge device in the future version.  [Proposed] Both Self Driving Simulation or Driving via Gaze Estimation is presently a proposed but non-canon part of the project , we’ll  be researching continuously on how to carry it out but for now these features totally rely on a high cost Simulator to channel the output data to attain Self Driving or Gaze Control. We’ll try to figure out how can we attain this with paying cost and if successfully found this feature will be made canon(i.e., an active part of Project Timeline).  Winding Up the whole description details listed, for clarity the last part of the project will be creating Live Feed and Benchmarking to make the project deployment-ready for an edge device in the future. During the entire project our focus will be on creating Project Model Structure, we’ll be working, testing and presenting on our current workstation. 
